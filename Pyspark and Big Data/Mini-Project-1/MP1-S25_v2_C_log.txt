/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/sbin/start-master.sh
starting org.apache.spark.deploy.master.Master, logging to /storage/work/ljc5809/MiniProj1/spark-ljc5809-org.apache.spark.deploy.master.Master-1-p-bc-5027.out
/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/bin/spark-class org.apache.spark.deploy.worker.Worker --work-dir /storage/work/ljc5809/MiniProj1 spark://p-bc-5027:7077
Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties
25/03/30 20:02:33 INFO Worker: Started daemon with process name: 1253987@p-bc-5027
25/03/30 20:02:33 INFO SignalUtils: Registering signal handler for TERM
25/03/30 20:02:33 INFO SignalUtils: Registering signal handler for HUP
25/03/30 20:02:33 INFO SignalUtils: Registering signal handler for INT
25/03/30 20:02:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 20:02:34 INFO SecurityManager: Changing view acls to: ljc5809
25/03/30 20:02:34 INFO SecurityManager: Changing modify acls to: ljc5809
25/03/30 20:02:34 INFO SecurityManager: Changing view acls groups to: 
25/03/30 20:02:34 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 20:02:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ljc5809); groups with view permissions: Set(); users  with modify permissions: Set(ljc5809); groups with modify permissions: Set()
25/03/30 20:02:34 INFO Utils: Successfully started service 'sparkWorker' on port 41425.
25/03/30 20:02:34 INFO Worker: Worker decommissioning not enabled.
25/03/30 20:02:35 INFO Worker: Starting Spark worker 10.6.8.37:41425 with 2 cores, 7.0 GiB RAM
25/03/30 20:02:35 INFO Worker: Running Spark version 3.3.0
25/03/30 20:02:35 INFO Worker: Spark home: /storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3
25/03/30 20:02:35 INFO ResourceUtils: ==============================================================
25/03/30 20:02:35 INFO ResourceUtils: No custom resources configured for spark.worker.
25/03/30 20:02:35 INFO ResourceUtils: ==============================================================
25/03/30 20:02:35 INFO Utils: Successfully started service 'WorkerUI' on port 8081.
25/03/30 20:02:35 INFO WorkerWebUI: Bound WorkerWebUI to 0.0.0.0, and started at http://p-bc-5027.2e.hpc.psu.edu:8081
25/03/30 20:02:35 INFO Worker: Connecting to master p-bc-5027:7077...
25/03/30 20:02:35 INFO TransportClientFactory: Successfully created connection to p-bc-5027/10.6.8.37:7077 after 105 ms (0 ms spent in bootstraps)
25/03/30 20:02:36 INFO Worker: Successfully registered with master spark://p-bc-5027:7077
/storage/icds/RISE/sw8/anaconda/anaconda3/lib/python3.9/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.1
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
25/03/30 20:02:48 INFO SparkContext: Running Spark version 3.3.0
25/03/30 20:02:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
25/03/30 20:02:49 INFO ResourceUtils: ==============================================================
25/03/30 20:02:49 INFO ResourceUtils: No custom resources configured for spark.driver.
25/03/30 20:02:49 INFO ResourceUtils: ==============================================================
25/03/30 20:02:49 INFO SparkContext: Submitted application: Mini Project #1 Freqent Port Sets Mining
25/03/30 20:02:49 INFO ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 1, script: , vendor: , memory -> name: memory, amount: 1024, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)
25/03/30 20:02:49 INFO ResourceProfile: Limiting resource is cpu
25/03/30 20:02:49 INFO ResourceProfileManager: Added ResourceProfile id: 0
25/03/30 20:02:49 INFO SecurityManager: Changing view acls to: ljc5809
25/03/30 20:02:49 INFO SecurityManager: Changing modify acls to: ljc5809
25/03/30 20:02:49 INFO SecurityManager: Changing view acls groups to: 
25/03/30 20:02:49 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 20:02:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ljc5809); groups with view permissions: Set(); users  with modify permissions: Set(ljc5809); groups with modify permissions: Set()
25/03/30 20:02:49 INFO Utils: Successfully started service 'sparkDriver' on port 46855.
25/03/30 20:02:49 INFO SparkEnv: Registering MapOutputTracker
25/03/30 20:02:49 INFO SparkEnv: Registering BlockManagerMaster
25/03/30 20:02:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
25/03/30 20:02:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
25/03/30 20:02:49 INFO SparkEnv: Registering BlockManagerMasterHeartbeat
25/03/30 20:02:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-173e40a6-12aa-41d6-8b31-3dc9fd3bfb33
25/03/30 20:02:49 INFO MemoryStore: MemoryStore started with capacity 434.4 MiB
25/03/30 20:02:49 INFO SparkEnv: Registering OutputCommitCoordinator
25/03/30 20:02:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
25/03/30 20:02:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://p-bc-5027:7077...
25/03/30 20:02:50 INFO TransportClientFactory: Successfully created connection to p-bc-5027/10.6.8.37:7077 after 53 ms (0 ms spent in bootstraps)
25/03/30 20:02:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20250330200250-0000
25/03/30 20:02:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38751.
25/03/30 20:02:50 INFO NettyBlockTransferService: Server created on p-bc-5027.2e.hpc.psu.edu:38751
25/03/30 20:02:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
25/03/30 20:02:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, p-bc-5027.2e.hpc.psu.edu, 38751, None)
25/03/30 20:02:50 INFO BlockManagerMasterEndpoint: Registering block manager p-bc-5027.2e.hpc.psu.edu:38751 with 434.4 MiB RAM, BlockManagerId(driver, p-bc-5027.2e.hpc.psu.edu, 38751, None)
25/03/30 20:02:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, p-bc-5027.2e.hpc.psu.edu, 38751, None)
25/03/30 20:02:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, p-bc-5027.2e.hpc.psu.edu, 38751, None)
25/03/30 20:02:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20250330200250-0000/0 on worker-20250330200234-10.6.8.37-41425 (10.6.8.37:41425) with 2 core(s)
25/03/30 20:02:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20250330200250-0000/0 on hostPort 10.6.8.37:41425 with 2 core(s), 1024.0 MiB RAM
25/03/30 20:02:50 INFO Worker: Asked to launch executor app-20250330200250-0000/0 for Mini Project #1 Freqent Port Sets Mining
25/03/30 20:02:50 INFO SecurityManager: Changing view acls to: ljc5809
25/03/30 20:02:50 INFO SecurityManager: Changing modify acls to: ljc5809
25/03/30 20:02:50 INFO SecurityManager: Changing view acls groups to: 
25/03/30 20:02:50 INFO SecurityManager: Changing modify acls groups to: 
25/03/30 20:02:50 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(ljc5809); groups with view permissions: Set(); users  with modify permissions: Set(ljc5809); groups with modify permissions: Set()
25/03/30 20:02:50 INFO ExecutorRunner: Launch command: "/usr/lib/jvm/java-11-openjdk-11.0.25.0.9-2.el8.x86_64/bin/java" "-cp" "/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/conf/:/storage/icds/RISE/sw8/spark-3.3.0-bin-hadoop3/jars/*" "-Xmx1024M" "-Dspark.driver.port=46855" "-XX:+IgnoreUnrecognizedVMOptions" "--add-opens=java.base/java.lang=ALL-UNNAMED" "--add-opens=java.base/java.lang.invoke=ALL-UNNAMED" "--add-opens=java.base/java.lang.reflect=ALL-UNNAMED" "--add-opens=java.base/java.io=ALL-UNNAMED" "--add-opens=java.base/java.net=ALL-UNNAMED" "--add-opens=java.base/java.nio=ALL-UNNAMED" "--add-opens=java.base/java.util=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent=ALL-UNNAMED" "--add-opens=java.base/java.util.concurrent.atomic=ALL-UNNAMED" "--add-opens=java.base/sun.nio.ch=ALL-UNNAMED" "--add-opens=java.base/sun.nio.cs=ALL-UNNAMED" "--add-opens=java.base/sun.security.action=ALL-UNNAMED" "--add-opens=java.base/sun.util.calendar=ALL-UNNAMED" "--add-opens=java.security.jgss/sun.security.krb5=ALL-UNNAMED" "org.apache.spark.executor.CoarseGrainedExecutorBackend" "--driver-url" "spark://CoarseGrainedScheduler@p-bc-5027.2e.hpc.psu.edu:46855" "--executor-id" "0" "--hostname" "10.6.8.37" "--cores" "2" "--app-id" "app-20250330200250-0000" "--worker-url" "spark://Worker@10.6.8.37:41425"
25/03/30 20:02:51 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20250330200250-0000/0 is now RUNNING
25/03/30 20:02:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
root
 |-- _c0: integer (nullable = true)
 |-- id: integer (nullable = true)
 |-- numports: integer (nullable = true)
 |-- lifetime: decimal(10,0) (nullable = true)
 |-- Bytes: integer (nullable = true)
 |-- Packets: integer (nullable = true)
 |-- average_packetsize: integer (nullable = true)
 |-- MinUniqueDests: integer (nullable = true)
 |-- MaxUniqueDests: integer (nullable = true)
 |-- MinUniqueDest24s: integer (nullable = true)
 |-- MaxUniqueDest24s: integer (nullable = true)
 |-- average_lifetime: decimal(10,0) (nullable = true)
 |-- mirai: boolean (nullable = true)
 |-- zmap: boolean (nullable = true)
 |-- masscan: boolean (nullable = true)
 |-- country: string (nullable = true)
 |-- traffic_types_scanned_str: string (nullable = true)
 |-- ports_scanned_str: string (nullable = true)
 |-- host_tags_per_censys: string (nullable = true)
 |-- host_services_per_censys: string (nullable = true)

25/03/30 20:21:41 INFO Worker: Asked to kill executor app-20250330200250-0000/0
25/03/30 20:21:41 INFO ExecutorRunner: Runner thread for executor app-20250330200250-0000/0 interrupted
25/03/30 20:21:41 INFO ExecutorRunner: Killing process!
25/03/30 20:21:41 INFO Worker: Executor app-20250330200250-0000/0 finished with state KILLED exitStatus 0
25/03/30 20:21:41 INFO ExternalShuffleBlockResolver: Clean up non-shuffle and non-RDD files associated with the finished executor 0
25/03/30 20:21:41 INFO ExternalShuffleBlockResolver: Executor is not registered (appId=app-20250330200250-0000, execId=0)
25/03/30 20:21:41 INFO Worker: Cleaning up local directories for application app-20250330200250-0000
25/03/30 20:21:41 INFO ExternalShuffleBlockResolver: Application app-20250330200250-0000 removed, cleanupLocalDirs = true
SPARK_MASTER_HOST=p-bc-5027
SPARK_MASTER_PORT=7077

real	19m16.706s
user	2m23.258s
sys	0m8.434s
