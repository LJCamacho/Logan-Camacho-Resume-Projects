---
title: "STAT380_HW7"
author: Name
output: html_document
date: "Due: Thursday, March 27, 2025 by 11:59 PM"
---

## Instructions

In class, we covered the basics of using k-fold cross validation for multiple linear regression models and kNN regression models. The activities that follow are meant to provide further practice with these concepts. NOTE: All required `library` commands **must be** included in the Front Matter section. If you include your `library` commands elsewhere in your code, you will be penalized.

At the conclusion to the activity, you should upload

1.  your .html file named LastnameFirstInitial_STAT380_HW7.html
2.  your .Rmd file named LastnameFirstInitial_STAT380_HW7.Rmd

## Learning Objectives

This assignment address aspects of the following learning objectives.

1.  Students will be able to load datasets from R packages and external sources into the R environment.
2.  Students will be able to identify R functions for and perform data wrangling tasks such as filtering observations based on a criterion, creating new variables, and restructuring data.
3.  Students will be able to use iteration (loops, apply, etc.) to perform tasks that must be repeated many times on a given dataset.
4.  Given a statistical learning method, students will be able to restructure the data for use in the corresponding R function.
5.  Fit a linear regression model using statistical software, including situations involving polynomial terms, categorical predictions, and/or interaction terms.
6.  Perform k-nearest neighbors regression using statistical software, including using cross validation to select the number of neighbors.
7.  Compare the ability of competing models to generalize for new data.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Front Matter - Clean Environment, Load Libraries, User Defined Functions

```{r}
rm(list = ls())
#Add libraries as needed

library(tidyverse)
library(palmerpenguins)
library(FNN)
#Read in dataset

Ins <- read.csv("C:/Users/Blungus/OneDrive - The Pennsylvania State University/2024-25/Spring Sem/Stat 380/Test CSV files/L02_Insurance_m.csv")
```

## Dataset

`L02_Insurance_m.csv` contains information about a number of health insurance policies. In particular, the data set contains some attributes of the policy holder (such as age, sex, etc.) and the total charges billed by the health care provider. Details about the variables included follow.

-   `age` - age of primary beneficiary
-   `sex` - sex of primary beneficiary
-   `bmi` - Body mass index, providing an understanding of body, weights that are relatively high or low relative to height, objective index of body weight (kg / m \^ 2) using the ratio of height to weight, ideally 18.5 to 24.9
-   `children` - Number of children covered by the health insurance policy (i.e., the number of dependents)
-   `smoker` - Status indicating whether the person is a smoker (options include 'yes' and 'no')
-   `region` - the beneficiary's residential area in the US (options include northeast, southeast, southwest, northwest).
-   `charges` - Individual medical costs as billed by health insurance

Our goal is to predict `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`).

## Problem 1 - Which method (multiple linear regression or kNN) produces the most accurate predictions for new data? To answer this, compare the performance of the methods using 10-fold cross validation.

a.  Prepare the data for implementing multiple linear regression and kNN. NOTE: I am not telling you the exact steps to take prepare the data, but if you unsure of the steps, see Problem 2 in HW5.
```{r}
#Remove an observation is there is at least one missing value among the row's varibles
Ins <- na.omit(Ins)
#Create indicator variables
Ins <- 
  Ins %>%
  mutate(smokes = ifelse(smoker == "yes", 1, 0),
         male = ifelse(sex == "male", 1, 0))

xvars <- c("age", "male", "bmi", "children", "smokes")
Ins[ , xvars] <- scale(Ins[ , xvars], center = TRUE, scale = TRUE)

```

b.  Assign the fold values and randomly permute them using a seed of 123. To make sure you are on the right path, the folds for the first few observations are provided below. (If you cannot see the results, check the .html file containing the knitted instructions.)

```{r}
#Create fold values
num_folds <- 10
folds <- cut(x = 1:nrow(Ins), breaks = num_folds, labels = FALSE)
#Permute
set.seed(123)
folds <- sample(folds) #randomly permute the fold values
set.seed(NULL)
```


c.  Implement 10-fold cross validation for the multiple linear regression model for predicting `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`). For each fold, calculate the Mean Square Error (MSE). Display the 10 MSE values **and** report the mean of the 10 MSE values.

    -   NOTE: Be sure to compute the MSE for each fold instead of the RMSE for each fold.
    -   NOTE: To implement 10-fold cross validation for this problem (and the next problem), you should write a loop for cycling through the folds as discussed in class instead of using a black box function that does everything behind the scenes.

```{r}

#Initialize
mseVec <- rep(NA, num_folds)


#loop
for(i in 1:num_folds){
  #Do Training/Validation split
  valInd <- which(folds == i)
  Validation <- Ins[valInd, ]
  Train <- Ins[-valInd, ]
  #Train Model
  modelRes <- lm(charges ~ age + male + bmi + children + smokes,
                 data = Train)
  
  
  
  #Calculate predictions on validation
  yhat <- predict(modelRes, newdata = Validation)
  
  
  #Find and store MSE 
  mseVec[i] <- mean((Validation$charges - yhat)^2)
}

mseVec
meanMSE <- mean(mseVec)
meanMSE
rootMeanMSE <- sqrt(meanMSE)



```

d.  Implement 10-fold cross validation for the k-Nearest Neighbors (kNN) regression model for predicting `charges` using `age`, `sex`, `bmi`, `children`, and smoking status (`smoker`). For the purposes of selecting k, consider values of k ranging from 1 to 50. Create a plot showing the Mean Mean Square Error (Mean MSE) for each fold as a function of k **and** state which value of k is optimal.

    -   NOTE: This can be challenging since we did not do this exact problem in the notes. The Lecture 11 Supplement document outlines the procedure and the code for you. You should start by reading the document.
    -   NOTE: When working in data science, you often have to read about a topic you have not previously implemented. Then, you try to implement it. That is the purpose of this question.

```{r}

#Initialize
mseVecKNN <- rep(NA, num_folds)

maxK <- 50

mseMat <- matrix(NA, nrow=num_folds, ncol= maxK)

#loop
for(i in 1:num_folds){
  #Do Training/Validation split
  valInd <- which(folds == i)
  Validation <- Ins[valInd, ]
  Train <- Ins[-valInd, ]
  

  for(j in 1:maxK){
  #Train the kNN Model and obtain Predictions
  knnRes <- knn.reg(train = Train[ , xvars, drop = FALSE],
                    test = Validation[ , xvars, drop = FALSE],
                    y = Train$charges,
                    k = j)
  
  #Calculate and store MSE
  mseMat[i,j] <- mean((Validation$charges - knnRes$pred)^2)
  
  } 
}

cvMSE <- colMeans(mseMat)
rootCVMSE <- sqrt(cvMSE)
#standard deviation for problem 2
sdMSE <- apply(mseMat, 2, sd)

#Create a temp DF for plotting in ggplot (k column with numbers 1 to maxK)
tempDF <- data.frame(k=1:maxK,
                     mse = cvMSE,
                     sd = sdMSE)

#Create plot showing mse as a function of k, circle lowest mse
ggplot(data = tempDF,
       mapping = aes(x=k,y=mse))+
  geom_line(color = "red")+
  labs(x = "Number of Nearest Neighbors (k)",
       y = "Root Mean Squared Error")+
  geom_point(data = tempDF[which.min(cvMSE), ],
             color = "blue",
             shape = 1,
             size = 5)
which.min(cvMSE)
```

For the 10-fold cross validation of kNN cross validation from 1:50 values of k, the optimal number of nearest neighbors based on the mean mean squared error per fold was found to be 16.

e.  Which method (multiple linear regression or kNN) produces the most accurate predictions for new data? **Explain your reasoning** by discussing the RMSE values associated with each method. NOTE: Although we calculated MSE values in Parts c. and d., compare the RMSE values in this question. Be sure to state the RMSE values.

```{r}
# mlm cv RMSE
rootMeanMSE

# kNN cv RMSE
rootCVMSE[which.min(cvMSE)]


```

Based on the results from mlm cross-validation vs kNN cross-validation, we got RMSE values of 6075.479 and 4944.077, respectively. Since the results from kNN (4944.077) CV was the lowest RMSE, we can conclude that the kNN cross-validation with 16 nearest neighbors produced the most accurate predictions for new data.


## Problem 2 - Plot

Recreate the plot shown below. (If you are not seeing the plot, be sure to open the .html file containing the instructions from Canvas.) The plot shows the plot created in Problem 1d (MSE as a function of k), but includes errors bars. For each value of k, you are to draw a **red** line that goes from (Mean MSE - standard deviation) to (Mean MSE + standard deviation). The Mean MSE is the mean of the MSE values within each fold. The standard deviation in this problem corresponds to the standard deviation of the MSE values within each fold. NOTE: While the error bars appear to have a similar length, the standard deviation is a different number for each value of k.

```{r}
#Create plot showing mse as a function of k, add error bars of mean MSE +- standard deviation
ggplot(data = tempDF,
       mapping = aes(x=k,y=mse))+
  geom_line(color = "black")+
  labs(x = "Number of Nearest Neighbors (k)",
       y = "Root Mean Squared Error")+
  geom_errorbar(aes(ymin=mse-sd, ymax=mse+sd), width=0, size = 1, color= "red")
```

